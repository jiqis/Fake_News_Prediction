{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/starting_kit/feature_extractor.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/starting_kit/feature_extractor.py\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "import scipy.sparse\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "def clean_str(sentence, stem=False): # choose whether to stemmer\n",
    "    english_stopwords = set(\n",
    "        [stopword for stopword in stopwords.words('english')])\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update([\"``\", \"`\", \"...\", \"[\", \"]\", \"'\", \",\", \".\"])\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                            x.lower() not in punctuation,\n",
    "                            [stemmer.stem(t.lower())\n",
    "                             for t in word_tokenize(sentence)\n",
    "                             if t.isalpha()])))\n",
    "\n",
    "    return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                        x.lower() not in punctuation,\n",
    "                        [t.lower() for t in word_tokenize(sentence)\n",
    "                         if t.isalpha()])))\n",
    "\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    if isinstance(s, float) or isinstance(s, int):\n",
    "        s = str(s)\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass \n",
    "    s = unicodedata.normalize('NFD', s) # NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    s = s.encode('ascii', 'ignore') # encodes a unicode string to ascii and ignores errors\n",
    "    s = s.decode('utf-8', 'ignore')\n",
    "    return str(s)\n",
    "\n",
    "def feature_represent_fit(X_df):\n",
    "    '''X_df: training dataframe\n",
    "       Output: dictionary of useful categories with more than 10 word frequence/\n",
    "               positions of dummy variables\n",
    "    '''\n",
    "    threshold = 10\n",
    "    col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "            'state', 'subjects']\n",
    "    category_dict = {}\n",
    "    position_dict = {}\n",
    "    for col in col_list:\n",
    "        i = 0\n",
    "        category_dict[col] = []\n",
    "        value_count = X_df[col].value_counts()\n",
    "        \n",
    "        for index, count in zip(value_count.index, value_count):\n",
    "            if count > threshold:\n",
    "                category_dict[col].append(index)\n",
    "            else:\n",
    "                X_df.loc[X_df[col]==index, col] = col + '_others'\n",
    "                i += 1\n",
    "        category_dict[col].append(col + '_others')\n",
    "        #print('    column {}, unique items before group {}, after group {}'.format(\n",
    "        #        col, len(value_count), len(value_count) - i + 1)) \n",
    "        # Get dummies of training data\n",
    "        dummy_col = pd.get_dummies(X_df[col])\n",
    "        # Position dictionary for each dummy variable\n",
    "        position_dict[col] = dict(zip(dummy_col.columns, np.arange(len(dummy_col.columns))))\n",
    "    return category_dict, position_dict\n",
    "\n",
    "def feature_represent_transform(category_dict, position_dict, X_df):\n",
    "        '''X_df: training or test dataframe'''\n",
    "        col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "                    'state', 'subjects']\n",
    "        df_dummy = pd.DataFrame()\n",
    "        for col in col_list:\n",
    "            X_df.loc[~X_df[col].isin(category_dict[col]), col] = col + '_others'\n",
    "            #print(len(set(X_df[col])), len(category_dict[col]))\n",
    "            dummy_col = pd.get_dummies(X_df[col])\n",
    "            # Elements in training set not in test set\n",
    "            complete = list(set(category_dict[col]) - set(X_df[col]))\n",
    "            if len(complete) != 0:\n",
    "            # Complete lacked columns by position\n",
    "                for w in complete:\n",
    "                    if int(position_dict[col][w]) < len(dummy_col.columns):\n",
    "                        dummy_col.insert(loc=int(position_dict[col][w]), column=w, value=0)\n",
    "                    else:\n",
    "                        dummy_col[w] = 0\n",
    "            df_dummy = pd.concat([df_dummy, dummy_col], axis=1)\n",
    "        return df_dummy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class FeatureExtractor(TfidfVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__(  # lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance\n",
    "            input='content', encoding='utf-8',\n",
    "            decode_error='ignore', strip_accents=None, lowercase=True,\n",
    "            preprocessor=None, tokenizer=None, analyzer='word',\n",
    "            stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "            ngram_range=(1, 3), max_df=1.0, min_df=1,\n",
    "            max_features=None, vocabulary=None, binary=False,\n",
    "            dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "            sublinear_tf=True)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "                        \n",
    "        self._feat = np.array([' '.join(clean_str(strip_accents_unicode(dd)))\n",
    "                                for dd in X_df.statement]) # statement\n",
    "        super(FeatureExtractor, self).fit(self._feat)\n",
    "        self.category_dict, self.position_dict = feature_represent_fit(X_df)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        return self.fit(X_df).transform(X_df)\n",
    "\n",
    "    def transform(self, X_df):       \n",
    "        # Tf-idf for statement\n",
    "        statement = np.array([' '.join(clean_str(strip_accents_unicode(dd)))\n",
    "                              for dd in X_df.statement])\n",
    "        check_is_fitted(self, '_feat', 'The tfidf vector is not fitted')\n",
    "        tfidf = super(FeatureExtractor, self).transform(statement)\n",
    "        # dummy coding\n",
    "        X_dummy = feature_represent_transform(self.category_dict, self.position_dict, X_df)\n",
    "        # Combine Tf-idf and dummy \n",
    "        array_dummy = np.array(X_dummy)\n",
    "        matrix_all = scipy.sparse.hstack([tfidf, array_dummy]).tocsr()\n",
    "        return matrix_all.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/starting_kit/classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/starting_kit/classifier.py\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf =MultinomialNB(fit_prior = False)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, (y >= 3) * 1)\n",
    " \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X) * 3 + 1\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pred = self.clf.predict(X)\n",
    "        probas = []\n",
    "        for i in range(pred.shape[0]):\n",
    "            if pred[i] ==1:\n",
    "                proba = [0, 0, 0, 0, 1, 0]\n",
    "            else:\n",
    "                proba = [0, 1, 0, 0, 0, 0]\n",
    "            probas.append(proba)\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178m\u001b[1mTesting Fake news: classify statements of public figures\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading train and test files from ./data ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading cv ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mTraining ./submissions/starting_kit ...\u001b[0m\n",
      "/Users/lingjin/anaconda3/lib/python3.5/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "./submissions/starting_kit/feature_extractor.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X_df.loc[X_df[col]==index, col] = col + '_others'\n",
      "./submissions/starting_kit/feature_extractor.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X_df.loc[~X_df[col].isin(category_dict[col]), col] = col + '_others'\n",
      "\u001b[38;5;178m\u001b[1mCV fold 0\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.227\u001b[0m  \u001b[38;5;218m0.427\u001b[0m  \u001b[38;5;218m0.577\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.277\u001b[0m  \u001b[38;5;150m0.568\u001b[0m  \u001b[38;5;150m0.777\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.185\u001b[0m  \u001b[38;5;105m0.511\u001b[0m  \u001b[38;5;105m0.692\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 1\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.226\u001b[0m  \u001b[38;5;218m0.426\u001b[0m  \u001b[38;5;218m0.574\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.268\u001b[0m  \u001b[38;5;150m0.558\u001b[0m  \u001b[38;5;150m0.765\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.191\u001b[0m  \u001b[38;5;105m0.451\u001b[0m  \u001b[38;5;105m0.599\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 2\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.226\u001b[0m  \u001b[38;5;218m0.426\u001b[0m  \u001b[38;5;218m0.576\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.265\u001b[0m  \u001b[38;5;150m0.554\u001b[0m  \u001b[38;5;150m0.760\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.183\u001b[0m  \u001b[38;5;105m0.423\u001b[0m  \u001b[38;5;105m0.567\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 3\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.228\u001b[0m  \u001b[38;5;218m0.436\u001b[0m  \u001b[38;5;218m0.588\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.274\u001b[0m  \u001b[38;5;150m0.566\u001b[0m  \u001b[38;5;150m0.773\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.195\u001b[0m  \u001b[38;5;105m0.444\u001b[0m  \u001b[38;5;105m0.613\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 4\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.227\u001b[0m  \u001b[38;5;218m0.433\u001b[0m  \u001b[38;5;218m0.588\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.271\u001b[0m  \u001b[38;5;150m0.565\u001b[0m  \u001b[38;5;150m0.774\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.200\u001b[0m  \u001b[38;5;105m0.438\u001b[0m  \u001b[38;5;105m0.598\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 5\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.241\u001b[0m  \u001b[38;5;218m0.432\u001b[0m  \u001b[38;5;218m0.577\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.263\u001b[0m  \u001b[38;5;150m0.557\u001b[0m  \u001b[38;5;150m0.762\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.210\u001b[0m  \u001b[38;5;105m0.437\u001b[0m  \u001b[38;5;105m0.602\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 6\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.224\u001b[0m  \u001b[38;5;218m0.430\u001b[0m  \u001b[38;5;218m0.581\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.263\u001b[0m  \u001b[38;5;150m0.552\u001b[0m  \u001b[38;5;150m0.751\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.183\u001b[0m  \u001b[38;5;105m0.415\u001b[0m  \u001b[38;5;105m0.585\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 7\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    acc   sacc  tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.223\u001b[0m  \u001b[38;5;218m0.426\u001b[0m  \u001b[38;5;218m0.575\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.272\u001b[0m  \u001b[38;5;150m0.568\u001b[0m  \u001b[38;5;150m0.777\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.240\u001b[0m  \u001b[38;5;105m0.486\u001b[0m  \u001b[38;5;105m0.655\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mMean CV scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore             acc            sacc           tfacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.228\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m0.0052\u001b[0m  \u001b[38;5;218m0.429\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m0.0037\u001b[0m  \u001b[38;5;218m0.579\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m0.0054\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m   \u001b[38;5;10m\u001b[1m0.269\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m0.005\u001b[0m  \u001b[38;5;150m0.561\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m0.005\u001b[0m9  \u001b[38;5;150m0.767\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m0.0086\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.198\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.0181\u001b[0m  \u001b[38;5;105m0.451\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.0302\u001b[0m  \u001b[38;5;105m0.614\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.0378\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mBagged scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;243mCouldn't re-order the score matrix..\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore   sacc\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.429\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.444\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ramp_test_submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
