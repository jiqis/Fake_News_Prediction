{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import word2vec\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Merge\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data_train = pd.read_csv(train_filename, sep='\\t')\n",
    "data_train = data_train.fillna('')\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filename = 'data/test.csv'\n",
    "data_test = pd.read_csv(test_filename, sep='\\t')\n",
    "data_test = data_test.fillna('')\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'edited_by', 'job', 'researched_by', 'source', 'state',\n",
       "       'statement', 'subjects', 'truth'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "                    'state', 'statement', 'subjects']\n",
    "\n",
    "statement_len_train = []\n",
    "for i in range(data_train.shape[0]):\n",
    "    x = []\n",
    "    statement_len_train_i = []\n",
    "    for dd in data_train[col_list].iloc[i].values:        \n",
    "        x.append(' '.join(strip_accents_unicode(clean_str(dd))))\n",
    "        statement_len_train_i.append(len(' '.join(clean_str(strip_accents_unicode(dd)))))\n",
    "    phrase = [' '.join(x)]\n",
    "    statement_len_train.append(sum(statement_len_train_i))\n",
    "    if i == 0:\n",
    "        str_list = phrase[:]\n",
    "        str_list_i = phrase[:]\n",
    "    else:\n",
    "        str_list_i = phrase[:]\n",
    "        str_list = np.append(str_list, str_list_i, axis=0)\n",
    "\n",
    "texts_train = str_list.copy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7569"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statement_len_test = []\n",
    "for i in range(data_test.shape[0]):\n",
    "    x = []\n",
    "    statement_len_test_i = []\n",
    "    for dd in data_test[col_list].iloc[i].values:        \n",
    "        x.append(' '.join(clean_str(strip_accents_unicode(dd))))\n",
    "        statement_len_test_i.append(len(' '.join(clean_str(strip_accents_unicode(dd)))))\n",
    "    phrase = [' '.join(x)]\n",
    "    statement_len_test.append(sum(statement_len_test_i))\n",
    "    if i == 0:\n",
    "        str_list = phrase[:]\n",
    "        str_list_i = phrase[:]\n",
    "    else:\n",
    "        str_list_i = phrase[:]\n",
    "        str_list = np.append(str_list, str_list_i, axis=0)\n",
    "\n",
    "texts_test = str_list.copy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ché Guevara \"wrote extensively about the superiority of white Europeans over people of African descent.\"\n"
     ]
    }
   ],
   "source": [
    "for i, statement in enumerate(data_train['statement']):\n",
    "    if u'\\xe9' in statement:\n",
    "        print(statement)\n",
    "        #statement.replace(u'\\xe9', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean statements and calculate length of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_train = []\n",
    "texts_test = []\n",
    "statement_len_train = []\n",
    "statement_len_test = []\n",
    "stops = set(stopwords.words('english'))   \n",
    "for statement in data_train['statement']:\n",
    "    #clean statements\n",
    "    statement_new = ' '.join(clean_str(statement))\n",
    "    statement_len_train.append(len(statement_new))\n",
    "    texts_train.append(statement_new)\n",
    "for statement in data_test['statement']:\n",
    "    #clean statements\n",
    "    statement_new = ' '.join(clean_str(statement))\n",
    "    statement_len_test.append(len(statement_new))\n",
    "    texts_test.append(statement_new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = int(np.percentile(statement_len_train, q=85))\n",
    "MAX_SEQUENCE_LENGTH = max(max(statement_len_train), max(statement_len_test))\n",
    "MAX_NB_WORDS = sum(statement_len_train) + sum(statement_len_test)\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = int(np.percentile(statement_len_train, q=85))\n",
    "MAX_SEQUENCE_LENGTH = max(statement_len_train)\n",
    "MAX_NB_WORDS = sum(statement_len_train) + sum(statement_len_test)\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283608"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9152"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "#fit index to word\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)\n",
    "#convert word list to sequences\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "#word index in embedding for word list, unique token\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/lingjin/Desktop/étude/data_camp/fake_news/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 3881\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM), dtype=np.float32)\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "                    'state', 'statement', 'subjects']\n",
    "        \n",
    "        for i in range(X_df.shape[0]):\n",
    "            x = []\n",
    "            for dd in X_df[col_list].iloc[i].values:        \n",
    "                x.append(' '.join(clean_str(strip_accents_unicode(dd))))\n",
    "                phrase = [' '.join(x)]\n",
    "            if i == 0:\n",
    "                str_list = phrase[:]\n",
    "                str_list_i = phrase[:]\n",
    "            else:\n",
    "                str_list_i = phrase[:]\n",
    "                str_list = np.append(str_list, str_list_i, axis=0)\n",
    "        return str_list\n",
    "        \n",
    "        X = np.array(str_list)\n",
    "        \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    if isinstance(s, float) or isinstance(s, int):\n",
    "        s = u'number'\n",
    "    else:\n",
    "        s = unicodedata.normalize('NFD', s) # NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    s = s.encode('ascii', 'ignore') # encodes a unicode string to ascii and ignores errors\n",
    "    try:\n",
    "        s = s.decode(\"utf-8\")\n",
    "    except TypeError:  \n",
    "        pass\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_accents_unicode(u'\\xe9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'\\xe9'.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data, validation data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = data_train['truth']\n",
    "y_test = data_test['truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train static shape: (7569, 345, 300)\n",
      "x_test static shape: (2891, 345, 300)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_label = train_label[indices]\n",
    "x_train = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in train_data])\n",
    "x_test = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in x_test])\n",
    "print(\"x_train static shape:\", x_train.shape)\n",
    "print(\"x_test static shape:\", x_test.shape)\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "x_train = x_train[:-nb_validation_samples][:]\n",
    "y_train = train_label[:-nb_validation_samples][:]\n",
    "x_val = x_train[-nb_validation_samples:][:]\n",
    "y_val = train_label[-nb_validation_samples:][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, num_classes=6)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=6)\n",
    "y_val = np_utils.to_categorical(y_val, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "filter_sizes = (2, 3, 4)\n",
    "num_filters = 128\n",
    "dropout_prob = 0.2\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Model input\n",
    "input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "model_input = Input(shape=input_shape, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CNN-static no need to use embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy learning-CNN-static\n",
    "\n",
    "https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 345, 300)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 344, 128)      76928       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 343, 128)      115328      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 342, 128)      153728      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 172, 128)      0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 171, 128)      0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 171, 128)      0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 22016)         0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 21888)         0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 21888)         0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 65792)         0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 65792)         0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            3289650     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 6)             306         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,635,940\n",
      "Trainable params: 3,635,940\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5677 samples, validate on 1892 samples\n",
      "Epoch 1/3\n",
      "5677/5677 [==============================] - 161s - loss: 1.7681 - acc: 0.1899 - val_loss: 1.7671 - val_acc: 0.2019\n",
      "Epoch 2/3\n",
      "5664/5677 [============================>.] - ETA: 0s - loss: 1.7542 - acc: 0.2149"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6b20b237c370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m model.fit(x_train, y_train, \n\u001b[1;32m     21\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m          epochs=3)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1196\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1198\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subconv = []\n",
    "for size in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=size,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1)(model_input)\n",
    "    maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "    flat = Flatten()(maxpool)\n",
    "    subconv.append(flat)\n",
    "model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "model = Dropout(dropout_prob)(model)\n",
    "model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, \n",
    "         validation_data=(x_val, y_val),\n",
    "         epochs=3)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.338983\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer concatenate_29 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.models.Sequential'>. Full input: [<keras.models.Sequential object at 0x1c376ea6d8>, <keras.models.Sequential object at 0x1c35c47358>, <keras.models.Sequential object at 0x1c36535e80>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    402\u001b[0m                           tf.SparseTensor)):\n\u001b[0;32m--> 403\u001b[0;31m         raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '\n\u001b[0m\u001b[1;32m    404\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.models.Sequential'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-b9d61edc3e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msubconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubconv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msubconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer concatenate_29 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.models.Sequential'>. Full input: [<keras.models.Sequential object at 0x1c376ea6d8>, <keras.models.Sequential object at 0x1c35c47358>, <keras.models.Sequential object at 0x1c36535e80>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "subconv = []\n",
    "for size in filter_sizes:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "    model.add(Convolution1D(filters=num_filters,\n",
    "                         kernel_size=size,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    subconv.append(model)\n",
    "conv = Sequential()\n",
    "conv.add(Merge(subconv, mode=\"concat\")) if len(subconv) > 1 else subconv[0]\n",
    "conv.add(Dropout(dropout_prob))\n",
    "conv.add(Dense(hidden_dims, activation=\"relu\"))\n",
    "conv.add(Dense(6, activation=\"sigmoid\"))\n",
    "\n",
    "conv.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(conv.summary())\n",
    "# fit the model\n",
    "conv.fit([x_train, x_train, x_train], y_train, \n",
    "         validation_data=([x_val, x_val, x_val], y_val),\n",
    "         epochs=3)\n",
    "# evaluate the model\n",
    "loss, accuracy = conv.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn():\n",
    "    # Matrix parameters\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    # Model Hyperparameters\n",
    "    num_filters = 128\n",
    "    dropout_prob = 0.2\n",
    "    hidden_dims = 50\n",
    "    filter_sizes = (2, 3, 4)\n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    # Model input\n",
    "    input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    model_input = Input(shape=input_shape, dtype='float32')\n",
    "    subconv = []\n",
    "    for size in filter_sizes:\n",
    "        conv = Convolution1D(filters=num_filters,\n",
    "                             kernel_size=sz,\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(model_input)\n",
    "        maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "        flat = Flatten()(maxpool)\n",
    "        subconv.append(flat)\n",
    "    model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "    model = Dropout(dropout_prob)(model)\n",
    "    model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "    model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5677, 6)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Flatten, Merge\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def model_cnn():\n",
    "    # Matrix parameters\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    # Model Hyperparameters\n",
    "    num_filters = 128\n",
    "    dropout_prob = 0.2\n",
    "    hidden_dims = 50\n",
    "    filter_sizes = (2, 3, 4)\n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    # Model input\n",
    "    input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    model_input = Input(shape=input_shape, dtype='float32')\n",
    "    subconv = []\n",
    "    for size in filter_sizes:\n",
    "        conv = Convolution1D(filters=num_filters,\n",
    "                             kernel_size=size,\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(model_input)\n",
    "        maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "        flat = Flatten()(maxpool)\n",
    "        subconv.append(flat)\n",
    "    model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "    model = Dropout(dropout_prob)(model)\n",
    "    model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "    model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = model_cnn()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_hot = np_utils.to_categorical(y, num_classes=6)[:]\n",
    "        return self.model.fit(X, y_hot, epochs=3)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Flatten, Merge\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def clean_str(sentence, stem=False): # choose whether to stemmer\n",
    "    english_stopwords = set(\n",
    "        [stopword for stopword in stopwords.words('english')])\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update([\"``\", \"`\", \"...\", \"[\", \"]\", \",\"])\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                            x.lower() not in punctuation,\n",
    "                            [stemmer.stem(t.lower())\n",
    "                             for t in word_tokenize(sentence)\n",
    "                             if t.isalpha()])))\n",
    "\n",
    "    return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                        x.lower() not in punctuation,\n",
    "                        [t.lower() for t in word_tokenize(sentence)\n",
    "                         if t.isalpha()])))\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    s = unicodedata.normalize('NFD', s) # NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    s = s.encode('ascii', 'ignore') # encodes a unicode string to ascii and ignores errors\n",
    "    s = s.decode(\"utf-8\")\n",
    "    return str(s)\n",
    "\n",
    "def import_sentence(X_df):\n",
    "    '''Preparation of sentence list for embedding matrix\n",
    "    '''\n",
    "    texts = []\n",
    "    #statement_len = []\n",
    "    stops = set(stopwords.words('english'))   \n",
    "    for statement in X_df.statement:\n",
    "        #clean statements\n",
    "        statement_new = ' '.join(clean_str(strip_accents_unicode(statement)))\n",
    "        #statement_len.append(len(statement_new))\n",
    "        texts.append(statement_new)\n",
    "    return texts\n",
    "\n",
    "def embedding_matrix(texts, path='.'):\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    #VALIDATION_SPLIT = 0.25 # Choose not to seperate a validation set\n",
    "    # Create word_index and sequences\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    # Fit index to word\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Convert word list to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Word index in embedding for word list, unique token\n",
    "    word_index = tokenizer.word_index\n",
    "    # Sequence data\n",
    "    data = pad_sequences(sequences, \n",
    "                         maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                         padding='post')\n",
    "    # Import Word2Vec embeddings\n",
    "    BASE_DIR = '/Users/lingjin/Desktop/étude/data_camp/fake_news/' # '.'\n",
    "    EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "    word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "            binary=True)\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM), dtype=np.float32)\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    \n",
    "    # Import embeddings to data\n",
    "    X = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in data])\n",
    "    return X\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    #def __init__(self):\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        '''Import sentences and pass it to embedding matrix\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        texts = import_sentence(X_df)\n",
    "        X = embedding_matrix(texts, path='.')\n",
    "        return X\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        texts = import_sentence(X_df)\n",
    "        X = embedding_matrix(texts, path='.')\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp\n",
    "\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "http://ben.bolte.cc/blog/2016/gensim.html\n",
    "\n",
    "https://github.com/chrisjmccormick/inspect_word2vec\n",
    "\n",
    "https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "\n",
    "http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb\n",
    "\n",
    "https://rare-technologies.com/word2vec-tutorial/\n",
    "\n",
    "quora pairs\n",
    "https://www.kaggle.com/c/quora-question-pairs/discussion/34355\n",
    "\n",
    "activation function\n",
    "https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions\n",
    "\n",
    "fake news\n",
    "https://github.com/sumeetkr/AwesomeFakeNews\n",
    "\n",
    "3 filter size\n",
    "https://github.com/fchollet/keras/issues/6547\n",
    "\n",
    "Dropout \n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
