{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import word2vec\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Merge\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data_train = pd.read_csv(train_filename, sep='\\t')\n",
    "data_train = data_train.fillna('')\n",
    "data_train['date'] = pd.to_datetime(data_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'data/test.csv'\n",
    "data_test = pd.read_csv(test_filename, sep='\\t')\n",
    "data_test = data_test.fillna('')\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "                    'state', 'statement', 'subjects']\n",
    "\n",
    "statement_len_train = []\n",
    "for i in range(data_train.shape[0]):\n",
    "    x = []\n",
    "    statement_len_train_i = []\n",
    "    for dd in data_train[col_list].iloc[i].values:        \n",
    "        x.append(' '.join(strip_accents_unicode(clean_str(dd))))\n",
    "        statement_len_train_i.append(len(' '.join(clean_str(strip_accents_unicode(dd)))))\n",
    "    phrase = [' '.join(x)]\n",
    "    statement_len_train.append(sum(statement_len_train_i))\n",
    "    if i == 0:\n",
    "        str_list = phrase[:]\n",
    "        str_list_i = phrase[:]\n",
    "    else:\n",
    "        str_list_i = phrase[:]\n",
    "        str_list = np.append(str_list, str_list_i, axis=0)\n",
    "\n",
    "texts_train = str_list.copy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_len_test = []\n",
    "for i in range(data_test.shape[0]):\n",
    "    x = []\n",
    "    statement_len_test_i = []\n",
    "    for dd in data_test[col_list].iloc[i].values:        \n",
    "        x.append(' '.join(clean_str(strip_accents_unicode(dd))))\n",
    "        statement_len_test_i.append(len(' '.join(clean_str(strip_accents_unicode(dd)))))\n",
    "    phrase = [' '.join(x)]\n",
    "    statement_len_test.append(sum(statement_len_test_i))\n",
    "    if i == 0:\n",
    "        str_list = phrase[:]\n",
    "        str_list_i = phrase[:]\n",
    "    else:\n",
    "        str_list_i = phrase[:]\n",
    "        str_list = np.append(str_list, str_list_i, axis=0)\n",
    "\n",
    "texts_test = str_list.copy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, statement in enumerate(data_train['statement']):\n",
    "    if u'\\xe9' in statement:\n",
    "        print(statement)\n",
    "        #statement.replace(u'\\xe9', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean statements and calculate length of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = []\n",
    "texts_test = []\n",
    "statement_len_train = []\n",
    "statement_len_test = []\n",
    "stops = set(stopwords.words('english'))   \n",
    "for statement in data_train['statement']:\n",
    "    #clean statements\n",
    "    statement_new = ' '.join(clean_str(statement))\n",
    "    statement_len_train.append(len(statement_new))\n",
    "    texts_train.append(statement_new)\n",
    "for statement in data_test['statement']:\n",
    "    #clean statements\n",
    "    statement_new = ' '.join(clean_str(statement))\n",
    "    statement_len_test.append(len(statement_new))\n",
    "    texts_test.append(statement_new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = int(np.percentile(statement_len_train, q=85))\n",
    "MAX_SEQUENCE_LENGTH = max(max(statement_len_train), max(statement_len_test))\n",
    "MAX_NB_WORDS = sum(statement_len_train) + sum(statement_len_test)\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = int(np.percentile(statement_len_train, q=85))\n",
    "MAX_SEQUENCE_LENGTH = max(statement_len_train)\n",
    "MAX_NB_WORDS = sum(statement_len_train) + sum(statement_len_test)\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "#fit index to word\n",
    "tokenizer.fit_on_texts(texts_train + texts_test)\n",
    "#convert word list to sequences\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "#word index in embedding for word list, unique token\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/lingjin/Desktop/Ã©tude/data_camp/fake_news/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM), dtype=np.float32)\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        col_list = ['job', 'edited_by', 'researched_by', 'source', \n",
    "                    'state', 'statement', 'subjects']\n",
    "        \n",
    "        for i in range(X_df.shape[0]):\n",
    "            x = []\n",
    "            for dd in X_df[col_list].iloc[i].values:        \n",
    "                x.append(' '.join(clean_str(strip_accents_unicode(dd))))\n",
    "                phrase = [' '.join(x)]\n",
    "            if i == 0:\n",
    "                str_list = phrase[:]\n",
    "                str_list_i = phrase[:]\n",
    "            else:\n",
    "                str_list_i = phrase[:]\n",
    "                str_list = np.append(str_list, str_list_i, axis=0)\n",
    "        return str_list\n",
    "        \n",
    "        X = np.array(str_list)\n",
    "        \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    if isinstance(s, float) or isinstance(s, int):\n",
    "        s = u'number'\n",
    "    else:\n",
    "        s = unicodedata.normalize('NFD', s) # NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    s = s.encode('ascii', 'ignore') # encodes a unicode string to ascii and ignores errors\n",
    "    try:\n",
    "        s = s.decode(\"utf-8\")\n",
    "    except TypeError:  \n",
    "        pass\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_accents_unicode(u'\\xe9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u'\\xe9'.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data, validation data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = data_train['truth']\n",
    "y_test = data_test['truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_label = train_label[indices]\n",
    "x_train = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in train_data])\n",
    "x_test = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in x_test])\n",
    "print(\"x_train static shape:\", x_train.shape)\n",
    "print(\"x_test static shape:\", x_test.shape)\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "x_train = x_train[:-nb_validation_samples][:]\n",
    "y_train = train_label[:-nb_validation_samples][:]\n",
    "x_val = x_train[-nb_validation_samples:][:]\n",
    "y_val = train_label[-nb_validation_samples:][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, num_classes=6)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=6)\n",
    "y_val = np_utils.to_categorical(y_val, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "filter_sizes = (2, 3, 4)\n",
    "num_filters = 128\n",
    "dropout_prob = 0.2\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Model input\n",
    "input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "model_input = Input(shape=input_shape, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-static no need to use embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy learning-CNN-static\n",
    "\n",
    "https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subconv = []\n",
    "for size in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=size,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1)(model_input)\n",
    "    maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "    flat = Flatten()(maxpool)\n",
    "    subconv.append(flat)\n",
    "model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "model = Dropout(dropout_prob)(model)\n",
    "model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, \n",
    "         validation_data=(x_val, y_val),\n",
    "         epochs=3)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subconv = []\n",
    "for size in filter_sizes:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "    model.add(Convolution1D(filters=num_filters,\n",
    "                         kernel_size=size,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    subconv.append(model)\n",
    "conv = Sequential()\n",
    "conv.add(Merge(subconv, mode=\"concat\")) if len(subconv) > 1 else subconv[0]\n",
    "conv.add(Dropout(dropout_prob))\n",
    "conv.add(Dense(hidden_dims, activation=\"relu\"))\n",
    "conv.add(Dense(6, activation=\"sigmoid\"))\n",
    "\n",
    "conv.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(conv.summary())\n",
    "# fit the model\n",
    "conv.fit([x_train, x_train, x_train], y_train, \n",
    "         validation_data=([x_val, x_val, x_val], y_val),\n",
    "         epochs=3)\n",
    "# evaluate the model\n",
    "loss, accuracy = conv.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn():\n",
    "    # Matrix parameters\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    # Model Hyperparameters\n",
    "    num_filters = 128\n",
    "    dropout_prob = 0.2\n",
    "    hidden_dims = 50\n",
    "    filter_sizes = (2, 3, 4)\n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    # Model input\n",
    "    input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    model_input = Input(shape=input_shape, dtype='float32')\n",
    "    subconv = []\n",
    "    for size in filter_sizes:\n",
    "        conv = Convolution1D(filters=num_filters,\n",
    "                             kernel_size=sz,\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(model_input)\n",
    "        maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "        flat = Flatten()(maxpool)\n",
    "        subconv.append(flat)\n",
    "    model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "    model = Dropout(dropout_prob)(model)\n",
    "    model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "    model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Flatten, Merge\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def model_cnn():\n",
    "    # Matrix parameters\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    # Model Hyperparameters\n",
    "    num_filters = 128\n",
    "    dropout_prob = 0.2\n",
    "    hidden_dims = 50\n",
    "    filter_sizes = (2, 3, 4)\n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    # Model input\n",
    "    input_shape = (MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    model_input = Input(shape=input_shape, dtype='float32')\n",
    "    subconv = []\n",
    "    for size in filter_sizes:\n",
    "        conv = Convolution1D(filters=num_filters,\n",
    "                             kernel_size=size,\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(model_input)\n",
    "        maxpool = MaxPooling1D(pool_size=2)(conv)\n",
    "        flat = Flatten()(maxpool)\n",
    "        subconv.append(flat)\n",
    "    model = concatenate(subconv, axis=1) if len(subconv) > 1 else subconv[0]\n",
    "    model = Dropout(dropout_prob)(model)\n",
    "    model = Dense(hidden_dims, activation='relu', input_dim=300)(model)\n",
    "    model_output = Dense(6, activation='sigmoid', input_dim=300)(model)\n",
    "    model = Model(model_input, model_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = model_cnn()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_hot = np_utils.to_categorical(y, num_classes=6)[:]\n",
    "        return self.model.fit(X, y_hot, epochs=3)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Flatten, Merge\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def clean_str(sentence, stem=False): # choose whether to stemmer\n",
    "    english_stopwords = set(\n",
    "        [stopword for stopword in stopwords.words('english')])\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update([\"``\", \"`\", \"...\", \"[\", \"]\", \",\"])\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                            x.lower() not in punctuation,\n",
    "                            [stemmer.stem(t.lower())\n",
    "                             for t in word_tokenize(sentence)\n",
    "                             if t.isalpha()])))\n",
    "\n",
    "    return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                        x.lower() not in punctuation,\n",
    "                        [t.lower() for t in word_tokenize(sentence)\n",
    "                         if t.isalpha()])))\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    s = unicodedata.normalize('NFD', s) # NFC, or 'Normal Form Composed' returns composed characters, NFD, 'Normal Form Decomposed' gives you decomposed, combined characters.\n",
    "    s = s.encode('ascii', 'ignore') # encodes a unicode string to ascii and ignores errors\n",
    "    s = s.decode(\"utf-8\")\n",
    "    return str(s)\n",
    "\n",
    "def import_sentence(X_df):\n",
    "    '''Preparation of sentence list for embedding matrix\n",
    "    '''\n",
    "    texts = []\n",
    "    #statement_len = []\n",
    "    stops = set(stopwords.words('english'))   \n",
    "    for statement in X_df.statement:\n",
    "        #clean statements\n",
    "        statement_new = ' '.join(clean_str(strip_accents_unicode(statement)))\n",
    "        #statement_len.append(len(statement_new))\n",
    "        texts.append(statement_new)\n",
    "    return texts\n",
    "\n",
    "def embedding_matrix(texts, path='.'):\n",
    "    MAX_SEQUENCE_LENGTH = 264\n",
    "    MAX_NB_WORDS = 671733# Calculate during analyses\n",
    "    EMBEDDING_DIM = 300\n",
    "    #VALIDATION_SPLIT = 0.25 # Choose not to seperate a validation set\n",
    "    # Create word_index and sequences\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    # Fit index to word\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # Convert word list to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    # Word index in embedding for word list, unique token\n",
    "    word_index = tokenizer.word_index\n",
    "    # Sequence data\n",
    "    data = pad_sequences(sequences, \n",
    "                         maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                         padding='post')\n",
    "    # Import Word2Vec embeddings\n",
    "    BASE_DIR = '/Users/lingjin/Desktop/Ã©tude/data_camp/fake_news/' # '.'\n",
    "    EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "    word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "            binary=True)\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "    # Create embedding matrix\n",
    "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM), dtype=np.float32)\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    \n",
    "    # Import embeddings to data\n",
    "    X = np.stack([np.stack([embedding_matrix[word] for word in sentence]) for sentence in data])\n",
    "    return X\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    #def __init__(self):\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        '''Import sentences and pass it to embedding matrix\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        texts = import_sentence(X_df)\n",
    "        X = embedding_matrix(texts, path='.')\n",
    "        return X\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        texts = import_sentence(X_df)\n",
    "        X = embedding_matrix(texts, path='.')\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp\n",
    "\n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "http://ben.bolte.cc/blog/2016/gensim.html\n",
    "\n",
    "https://github.com/chrisjmccormick/inspect_word2vec\n",
    "\n",
    "https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "\n",
    "http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb\n",
    "\n",
    "https://rare-technologies.com/word2vec-tutorial/\n",
    "\n",
    "quora pairs\n",
    "https://www.kaggle.com/c/quora-question-pairs/discussion/34355\n",
    "\n",
    "activation function\n",
    "https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions\n",
    "\n",
    "fake news\n",
    "https://github.com/sumeetkr/AwesomeFakeNews\n",
    "\n",
    "3 filter size\n",
    "https://github.com/fchollet/keras/issues/6547\n",
    "\n",
    "Dropout \n",
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
